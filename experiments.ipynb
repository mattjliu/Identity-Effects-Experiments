{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import pickle\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras import backend as K\n",
    "from string import ascii_uppercase as letters\n",
    "import random\n",
    "from random import randint\n",
    "from scipy.stats import ortho_group\n",
    "from pathlib import Path\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define binary distributed enoding (j active bits)\n",
    "def get_distr_j_encoding(k=26, j=3):\n",
    "    array_dict = {}\n",
    "    str_dict = {}\n",
    "    for i,letter in enumerate(letters):\n",
    "        indexes = np.random.choice(a=k, size=j, replace=False)\n",
    "        encoding = np.array([1 if i in indexes else 0 for i in range(k)])\n",
    "        encoding_str = ''.join(str(b) for b in encoding)\n",
    "        while encoding_str in str_dict.values():\n",
    "            indexes = np.random.choice(a=k, size=j, replace=False)\n",
    "            encoding = np.array([1 if i in indexes else 0 for i in range(k)])\n",
    "            encoding_str = ''.join(str(b) for b in encoding)\n",
    "        array_dict[letter] = encoding\n",
    "        str_dict[letter] = encoding_str\n",
    "    return array_dict, str_dict\n",
    "        \n",
    "# Define normal encoding\n",
    "def get_normal_encoding(k=26):\n",
    "    array_dict = {}\n",
    "    str_dict = {}\n",
    "    matrix = np.random.normal(loc=0,size=(k,k)) / (k**0.5)\n",
    "    for i,row in enumerate(matrix):\n",
    "        encoding_str = ''.join(str(b) for b in row)\n",
    "        array_dict[letters[i]] = row\n",
    "        str_dict[letters[i]] = encoding_str\n",
    "    return array_dict, str_dict\n",
    "\n",
    "# Define one hot encoding\n",
    "def get_one_hot_encoding(k=26):\n",
    "    array_dict = {}\n",
    "    str_dict = {}\n",
    "    for i,letter in enumerate(letters):\n",
    "        encoding = np.zeros(k)\n",
    "        encoding[i] = 1\n",
    "        encoding_str = ''.join(str(int(b)) for b in encoding)\n",
    "        \n",
    "        array_dict[letter] = encoding\n",
    "        str_dict[letter] = encoding_str\n",
    "    return array_dict, str_dict\n",
    "\n",
    "# Define the distributed encoding\n",
    "def get_distr_encoding(k=26):\n",
    "    array_dict = {}\n",
    "    str_dict = {}\n",
    "    for i,letter in enumerate(letters):\n",
    "        encoding = np.random.randint(0,2,k)\n",
    "        encoding_str = ''.join(str(b) for b in encoding)\n",
    "        while encoding_str in str_dict.values():\n",
    "            encoding = np.random.randint(0,2,k)\n",
    "            encoding_str = ''.join(str(b) for b in encoding)\n",
    "        array_dict[letter] = encoding\n",
    "        str_dict[letter] = encoding_str\n",
    "    return array_dict, str_dict\n",
    "\n",
    "# Define Haar encoding\n",
    "def get_orthogonal_encoding(k=26):\n",
    "    matrix = ortho_group.rvs(dim=k)\n",
    "    array_dict = {}\n",
    "    str_dict = {}\n",
    "    for i,row in enumerate(matrix):\n",
    "        array_dict[letters[i]] = row\n",
    "        str_dict[letters[i]] = ''.join(str(c) for c in row)\n",
    "    return array_dict, str_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Alpha experiments #####################################\n",
    "# Creates encoded words\n",
    "def create_encoded_words(encoding, words):\n",
    "    x_test_list = [np.append(encoding[w[0]],encoding[w[1]]) for w in words]\n",
    "    return np.array(x_test_list)\n",
    "\n",
    "# Creates test set words\n",
    "def create_test_words():                \n",
    "    words = ['AA','AB']\n",
    "    for x in letters[24:26]:\n",
    "        for y in letters[24:26]:\n",
    "            words.append(x + y)\n",
    "\n",
    "    words.append(letters[randint(0,23)] + 'Y')\n",
    "    words.append(letters[randint(0,23)] + 'Z')\n",
    "    return words\n",
    "\n",
    "# Create x_train and y_train from the encoding\n",
    "def create_train_words():\n",
    "    good_words = np.array([i + i for i in letters[:24]])\n",
    "    bad_words = np.array([x + y for x in letters[:24] for y in letters[:24] if x!=y])\n",
    "    \n",
    "    x_train = good_words\n",
    "    x_train = np.append(x_train, np.random.choice(bad_words,48,replace=False))\n",
    "    y_train = np.append(np.ones(24), np.zeros(48))\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "# Create the alphabet datasets\n",
    "def get_alpha_datasets(encoding_f, args={}, seed=1, model_type='FFWD'):\n",
    "    assert model_type in ['FFWD', 'LSTM']\n",
    "    random.seed(seed)\n",
    "    \n",
    "    test_words = create_test_words()\n",
    "    train_words, y_train = create_train_words()\n",
    "\n",
    "    encoding, str_encoding = encoding_f(**args)\n",
    "\n",
    "    x_test = create_encoded_words(encoding, test_words)\n",
    "    x_train = create_encoded_words(encoding, train_words)\n",
    "\n",
    "    # Add bad word to test set (depending on x_train)\n",
    "    x_test[1] = x_train[np.where(y_train == 0)[0][0]]\n",
    "    y_test = np.array([1,0,1,0,0,1,0,0])\n",
    "        \n",
    "    if model_type == 'LSTM':\n",
    "        x_train = x_train.reshape(len(x_train), 2, x_train.shape[1]//2)\n",
    "        x_test = x_test.reshape(len(x_test), 2, x_test.shape[1]//2)\n",
    "        y_train = y_train.reshape(len(y_train),1)\n",
    "        y_test = y_test.reshape(len(y_test),1)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test), (train_words, test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### MNIST experiments #####################################\n",
    "def create_datasets(df, n_examples=10):\n",
    "    train_set_list = []\n",
    "    for n in range(8):\n",
    "        train_set_list.append(df[df['label'] == n].sample(n_examples,replace=False))\n",
    "    \n",
    "    train_set = pd.concat(train_set_list)\n",
    "    return train_set, df[~df.index.isin(train_set.index)]\n",
    "\n",
    "def create_train_nums(train_df):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    i = 0\n",
    "    for row1 in train_df.iterrows():\n",
    "        for row2 in train_df.iterrows():\n",
    "            encoding1 = row1[1][[str(i) for i in range(10)]].values\n",
    "            encoding2 = row2[1][[str(i) for i in range(10)]].values\n",
    "            \n",
    "            if row1[1]['label'] == row2[1]['label']:\n",
    "                x_train.append(np.concatenate([encoding1, encoding2]))\n",
    "                y_train.append(1)\n",
    "            else:\n",
    "                if (i % 7 == 0) or (i % 7 ==1):\n",
    "                    x_train.append(np.concatenate([encoding1, encoding2]))\n",
    "                    y_train.append(0)\n",
    "            i+=1\n",
    "    \n",
    "    return np.array(x_train), np.array(y_train)\n",
    "\n",
    "def create_test_nums(train_df,valid_df):\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    labels = []\n",
    "    numbers = [str(i) for i in range(10)]\n",
    "    \n",
    "    # Train data encodings\n",
    "    random_num = random.randint(0,7)\n",
    "    df = train_df[train_df['label'] == random_num].sample(2,replace=False)\n",
    "    x_test.append(np.concatenate([df.iloc[0][numbers].values, df.iloc[1][numbers].values]))\n",
    "    y_test.append(1)\n",
    "    labels.append(\"XX\")\n",
    "    \n",
    "    random_num = random.randint(0,7)\n",
    "    encoding1_train = train_df[train_df['label'] == random_num].sample(1,replace=False).iloc[0][numbers].values\n",
    "    encoding2_train = train_df[train_df['label'] != random_num].sample(1,replace=False).iloc[0][numbers].values\n",
    "    x_test.append(np.concatenate([encoding1_train, encoding2_train]))\n",
    "    y_test.append(0)\n",
    "    labels.append(\"XY\")\n",
    "    \n",
    "    # Validation data encodings\n",
    "    random_num = random.randint(0,7)\n",
    "    df = valid_df[valid_df['label'] == random_num].sample(2,replace=False)\n",
    "    x_test.append(np.concatenate([df.iloc[0][numbers].values, df.iloc[1][numbers].values]))\n",
    "    y_test.append(1)\n",
    "    labels.append(\"X'X'\")\n",
    "    \n",
    "    random_num = random.randint(0,7)\n",
    "    encoding1_valid = valid_df[valid_df['label'] == random_num].sample(1,replace=False).iloc[0][numbers].values\n",
    "    encoding2_valid = valid_df[valid_df['label'] != random_num].sample(1,replace=False).iloc[0][numbers].values\n",
    "    x_test.append(np.concatenate([encoding1_valid, encoding2_valid]))\n",
    "    y_test.append(0)\n",
    "    labels.append(\"X'Y'\")\n",
    "    \n",
    "    eight_nine_df = pd.concat([\n",
    "        valid_df[valid_df['label'] == 8].sample(1,replace=False),\n",
    "        valid_df[valid_df['label'] == 9].sample(1,replace=False)\n",
    "    ])\n",
    "    \n",
    "    for row1 in eight_nine_df.iterrows():\n",
    "        for row2 in eight_nine_df.iterrows():\n",
    "            encoding8 = row1[1][numbers].values\n",
    "            encoding9 = row2[1][numbers].values\n",
    "            x_test.append(np.concatenate([encoding8, encoding9]))\n",
    "            \n",
    "            if row1[1]['label'] == row2[1]['label']:\n",
    "                y_test.append(1)\n",
    "            else:\n",
    "                y_test.append(0)\n",
    "                \n",
    "            labels.append(str(int(row1[1]['label'])) + str(int(row2[1]['label'])))\n",
    "            \n",
    "    x_test.append(np.concatenate([encoding1_valid, encoding8]))\n",
    "    y_test.append(0)\n",
    "    labels.append(\"X'8\")\n",
    "    \n",
    "    x_test.append(np.concatenate([encoding1_valid, encoding9]))\n",
    "    y_test.append(0)\n",
    "    labels.append(\"X'9\")\n",
    "    \n",
    "    return np.array(x_test), np.array(y_test), labels\n",
    "\n",
    "def get_mnist_datasets(cv_pred_df, model_type='FFWD', seed=1):\n",
    "    assert model_type in ['FFWD', 'LSTM']\n",
    "    random.seed(seed)\n",
    "    \n",
    "    train_df, valid_df = create_datasets(cv_pred_df)\n",
    "    x_test, y_test, labels = create_test_nums(train_df, valid_df)\n",
    "    x_train, y_train = create_train_nums(train_df)\n",
    "    \n",
    "    if model_type == 'LSTM':\n",
    "        x_train = x_train.reshape(len(x_train), 2, x_train.shape[1]//2)\n",
    "        x_test = x_test.reshape(len(x_test), 2, x_test.shape[1]//2)\n",
    "        y_train = y_train.reshape(len(y_train),1)\n",
    "        y_test = y_test.reshape(len(y_test),1)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function recompiles model for for each training instance\n",
    "# n: Number of hidden layers\n",
    "# activation: String denoting the activation function for hidden layers \n",
    "# initializer: keras.initializers object used for initializing weights\n",
    "# optimizer: keras.optimizers object used during training\n",
    "# dropout: dropout percentage\n",
    "def create_model(n=0, units=2**5, initializer=keras.initializers.RandomNormal(mean=0), \n",
    "                 optimizer=keras.optimizers.Adam(), dropout=0.75, model_type='FFWD', activation='relu', input_dim=26):\n",
    "    assert model_type in ['FFWD', 'LSTM']\n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_type == 'FFWD':\n",
    "        model.add(Dense(units=units, \n",
    "                        activation=activation,\n",
    "                        kernel_initializer=initializer,\n",
    "                        bias_initializer=initializer,\n",
    "                        input_dim=input_dim*2))\n",
    "        for j in range(n):\n",
    "            model.add(Dense(units=units,\n",
    "                            activation=activation,\n",
    "                            kernel_initializer=initializer,\n",
    "                            bias_initializer=initializer))\n",
    "            \n",
    "        model.add(Dense(units=1, \n",
    "                        activation='sigmoid',\n",
    "                        kernel_initializer=initializer,\n",
    "                        bias_initializer=initializer))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    else:\n",
    "        if n>0:\n",
    "            for j in range(n):\n",
    "                model.add(LSTM(units, \n",
    "                               input_shape=(2, input_dim), \n",
    "                               return_sequences=True,\n",
    "                               kernel_initializer=initializer,\n",
    "                               bias_initializer=initializer))\n",
    "                model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(LSTM(units, \n",
    "                       input_shape=(2, input_dim),\n",
    "                       kernel_initializer=initializer,\n",
    "                       bias_initializer=initializer))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid',\n",
    "                        kernel_initializer=initializer,\n",
    "                        bias_initializer=initializer))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function runs the experiments and saves both the training histories and test set results\n",
    "# test_words: List of test words (each word as a string)\n",
    "# encoding_f: Function object for creating encodings\n",
    "# out_folder: Filenames for saving (1,2,3 layers in order)\n",
    "# epochs_list: Number of epochs to train for 1,2 and 3 layers respectively\n",
    "# units_list: Number of units to use for 1,3 and 3 layers respectively\n",
    "# n_experiments: Number of experiments for each network architecture (default 40)\n",
    "# seed: Random seed (default 1)\n",
    "# optimizer: keras optimizer object\n",
    "# initializer: keras initializer object\n",
    "def run_experiment(out_folder, epochs_list, units_list, encoding_f=None, encoding_args={}, n_experiments=40, seed=1, model_type='FFWD',\n",
    "                   data_type='ALPHA', cv_pred_df=None, learning_rate=0.001, logfile='log.pkl', optimizer_type='ADAM'):\n",
    "    \n",
    "    assert model_type in ['FFWD','LSTM']\n",
    "    assert data_type in ['ALPHA', 'MNIST']\n",
    "    assert optimizer_type in ['ADAM', 'SGD']\n",
    "    \n",
    "    # Fit models and create dataframes\n",
    "    df_list = []\n",
    "    \n",
    "    if data_type == 'ALPHA':\n",
    "        assert encoding_f is not None\n",
    "        _, _, (_, test_words) = get_alpha_datasets(encoding_f, encoding_args, model_type=model_type)\n",
    "        test_labels = test_words.copy()\n",
    "        test_labels[1] = 'xy'\n",
    "    else:\n",
    "        assert cv_pred_df is not None\n",
    "        _, _, test_nums = get_mnist_datasets(cv_pred_df, model_type=model_type)\n",
    "        test_labels = test_nums.copy()\n",
    "    \n",
    "    # Start main loop\n",
    "    for i in range(3):\n",
    "        random.seed(seed)\n",
    "        history_list = []\n",
    "        df = pd.DataFrame(columns=test_labels)\n",
    "        for j in range(n_experiments):\n",
    "            K.clear_session()\n",
    "            \n",
    "            # Create x_train words\n",
    "            if data_type == 'ALPHA':\n",
    "                (x_train, y_train), (x_test, y_test), _ = get_alpha_datasets(encoding_f, \n",
    "                                                                             encoding_args, \n",
    "                                                                             model_type=model_type)\n",
    "            else:\n",
    "                (x_train, y_train), (x_test, y_test), _ = get_mnist_datasets(cv_pred_df,\n",
    "                                                                             model_type=model_type)\n",
    "            \n",
    "            if optimizer_type == 'ADAM':\n",
    "                optimizer = keras.optimizers.Adam(learning_rate)\n",
    "            else:\n",
    "                optimizer = keras.optimizers.SGD(learning_rate)\n",
    "            initializer = keras.initializers.RandomNormal(mean=0)\n",
    "\n",
    "            input_dim = 26 if data_type == 'ALPHA' else 10\n",
    "            model = create_model(i, units=units_list[i], \n",
    "                                 initializer=initializer, optimizer=optimizer, \n",
    "                                 model_type=model_type, input_dim=input_dim)\n",
    "            history = model.fit(x_train, y_train, \n",
    "                                epochs=epochs_list[i], batch_size=len(x_train), verbose=0, shuffle=True, \n",
    "                                validation_data=(x_test,y_test))\n",
    "            history_list.append(history.history)\n",
    "            df = df.append(pd.DataFrame(model.predict(x_test).T, columns=test_labels), ignore_index=True)\n",
    "\n",
    "        save_dict = {}\n",
    "        save_dict['df_list'] = df\n",
    "        save_dict['history_list'] = history_list\n",
    "\n",
    "        try:\n",
    "            os.makedirs(out_folder)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        with open(os.path.join(out_folder, f'{epochs_list[i]}it_{units_list[i]}units_{i+1}layers.pkl'), 'wb') as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "            f.close()\n",
    "            \n",
    "    if data_type == 'ALPHA':\n",
    "        dump_dict = dict(epochs=epochs_list, units=units_list, \n",
    "                         encoding=encoding_f.__name__, \n",
    "                         n_experiments=n_experiments,\n",
    "                         data_type=data_type,\n",
    "                         model_type=model_type,\n",
    "                         optimizer_type=optimizer_type,\n",
    "                         optimizer=optimizer.get_config())\n",
    "    else:\n",
    "        dump_dict = dict(epochs=epochs_list, units=units_list, \n",
    "                         n_experiments=n_experiments,\n",
    "                         data_type=data_type,\n",
    "                         model_type=model_type,\n",
    "                         optimizer_type=optimizer_type,\n",
    "                         optimizer=optimizer.get_config())\n",
    "    \n",
    "    with open(os.path.join(out_folder, logfile), 'wb') as f:\n",
    "              pickle.dump(dump_dict, f)\n",
    "              f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a run\n",
    "cv_pred_df = pd.read_csv('CV_output/CV_predictions_train_97ep.csv')\n",
    "run_experiment(cv_pred_df=cv_pred_df, out_folder='runs/MNIST/alpha_1/LSTM/97ep',\n",
    "               epochs_list=[1000,1000,1000], units_list=[32,32,32], n_experiments=40,\n",
    "               learning_rate=0.01, data_type='MNIST', model_type='LSTM')\n",
    "\n",
    "run_experiment(encoding_f=get_one_hot_encoding, out_folder=f'runs/ALPHA/LSTM/{get_one_hot_encoding.__name__}', \n",
    "               epochs_list=[1000,1000,1000], units_list=[32,32,32], n_experiments=40, \n",
    "               learning_rate=0.01, model_type='LSTM', data_type='ALPHA', optimizer_type='ADAM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
